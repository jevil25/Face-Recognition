{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1080640,"sourceType":"datasetVersion","datasetId":601762},{"sourceId":7815059,"sourceType":"datasetVersion","datasetId":4578095},{"sourceId":7824023,"sourceType":"datasetVersion","datasetId":4584475},{"sourceId":7824151,"sourceType":"datasetVersion","datasetId":4584569}],"dockerImageVersionId":29869,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom collections import defaultdict\n\nimport scikitplot\nimport seaborn as sns\nfrom matplotlib import pyplot\n\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, GlobalMaxPool2D\nfrom tensorflow.keras.layers import TimeDistributed, LSTM, Bidirectional\nfrom tensorflow.keras.layers import Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\n\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = \"../input/ck48-5-emotions/CK+48/\"\nFER13_PATH = \"../input/fer2013/dataset200224.xlsx\"\n\nfor dir_ in os.listdir(INPUT_PATH):\n    count = 0\n    for f in os.listdir(INPUT_PATH + dir_ + \"/\"):\n        count += 1\n    print(f\"{dir_} has {count} number of images\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nfer_data = pd.read_excel(FER13_PATH)\nprint(fer_data)\nfer_data[\"emotion\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`sadness` and `fear` has very low number of images as compared to other classes","metadata":{}},{"cell_type":"code","source":"FER13_EMOTIONS = [\"anger\",\n                \"disgust\",\n                \"fear\",\n                \"happy\",\n                \"sadness\",\n                \"surprise\",\n                \"neutral\"]\nTOP_EMOTIONS = FER13_EMOTIONS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing\n\nI first make the data compatible for neural nets","metadata":{}},{"cell_type":"code","source":"def directimg2array(path):\n    img = cv2.imread(path, 0)\n    return img\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_PATH = \"../input/ck48-5-emotions/CK+48/\"\n\ndata = defaultdict(str)\nfor dir_ in os.listdir(INPUT_PATH):\n    if dir_ in TOP_EMOTIONS:\n        data[dir_] = []\n        for f in os.listdir(INPUT_PATH + dir_ + \"/\"):\n            data[dir_].append(f)\n    else:\n        data[dir_] = []\n\n# data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import to_categorical\ndef CRNO(df):\n    df['pixels'] = df['pixels'].apply(lambda pixel_sequence: [int(pixel) for pixel in pixel_sequence.split()])\n    data_X = np.array(df['pixels'].tolist(),dtype='float32').reshape(-1,48,48,1)/255.0\n    data_Y = [int(emotion) for emotion in df[\"emotion\"]]\n    return data_X, data_Y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(fer_data[\"emotion\"].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X,train_Y = CRNO(fer_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for emotion in data:\n    data[emotion] = [directimg2array(path = INPUT_PATH + emotion + \"/\"+v) for v in data[emotion]]\n\ndata[\"disgust\"]=[]\ndata[\"neutral\"]=[] \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reshape to merge with ck+\ntrain_X = np.reshape(train_X,(len(train_X),48,48))\n\nfor index,fer in enumerate(train_X):\n    data[FER13_EMOTIONS[train_Y[index]]]+=[fer]\nprint(data[\"happy\"][0].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# min_samples = min(len(v) for v in data.values())\n\n# for k, v in data.items():\n#     if len(v) > min_samples:\n#         data[k] = v[:min_samples]\n\nfor k, v in data.items():\n    print(f\"{k} has {len(v)} samples\")\n    print(v[0].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"surprise = np.stack(data[\"surprise\"], axis=0)\nsurprise = surprise.reshape(len(data[\"surprise\"]),48,48,1)\n\nhappy = np.stack(data[\"happy\"], axis=0)\nhappy = happy.reshape(len(data[\"happy\"]),48,48,1)\n\nanger = np.stack(data[\"anger\"], axis=0)\nanger = anger.reshape(len(data[\"anger\"]),48,48,1)\n\nsadness = np.stack(data[\"sadness\"], axis=0)\nsadness = sadness.reshape(len(data[\"sadness\"]),48,48,1)\n\nfear = np.stack(data[\"fear\"], axis=0)\nfear = fear.reshape(len(data[\"fear\"]),48,48,1)\n\ndisgust = np.stack(data[\"disgust\"], axis=0)\ndisgust = disgust.reshape(len(data[\"disgust\"]),48,48,1)\n\nneutral = np.stack(data[\"neutral\"], axis=0)\nneutral = neutral.reshape(len(data[\"neutral\"]),48,48,1)\n\nX = np.concatenate((anger, disgust, fear, happy,sadness, surprise, neutral))\ny = np.concatenate((np.array([0]*len(data[\"anger\"])), np.array([1]*len(data[\"disgust\"])), np.array([2]*len(data[\"fear\"])), np.array([3]*len(data[\"happy\"])), np.array([4]*len(data[\"sadness\"])),  np.array([5]*len(data[\"surprise\"])),  np.array([6]*len(data[\"neutral\"]))))\ny = np_utils.to_categorical(y)\n\nX.shape, y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(happy.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list of integers from 0 to 6\nindices = list(range(7))\n\n# Map integers to emotions using list comprehension\nlabel_encoder= [FER13_EMOTIONS[i] for i in indices]\n\nlabel_encoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train, X_test, y_test = train_test_split(X, y, train_size=0.8, stratify=y, shuffle=True, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nnum = random.random()\nnp.random.seed(int(num*100))\nanger_idx = np.random.choice(np.where(X_test[:, 0]==1)[0], size=1)\ndisgust_idx = np.random.choice(np.where(X_test[:, 1]==1)[0], size=1)\nfear_idx = np.random.choice(np.where(X_test[:, 2]==1)[0], size=1)\nhappy_idx = np.random.choice(np.where(X_test[:, 3]==1)[0], size=1)\nsad_idx = np.random.choice(np.where(X_test[:, 4]==1)[0], size=1)\nsurprise_idx = np.random.choice(np.where(X_test[:, 5]==1)[0], size=1)\nneutral_idx = np.random.choice(np.where(X_test[:, 6]==1)[0], size=1)\n\n\nfig = pyplot.figure(1, (6,13))\ni = 0\nfor name, idx in zip(FER13_EMOTIONS, [anger_idx,disgust_idx, fear_idx, happy_idx, sad_idx, surprise_idx,neutral_idx]):\n    i += 1\n    ax = pyplot.subplot(5,3,i)\n    sample_img = X_train[idx]\n    sample_img = np.reshape(sample_img, (48,48))\n    ax.imshow(sample_img, cmap='gray')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data normalization\nX_train = X_train / 255.\nX_test = X_test / 255.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_dcnn(input_shape, show_arch=True):\n    \"\"\"\n    This is a Deep Convolutional Neural Network (DCNN). For generalization purpose I used dropouts in regular intervals.\n    I used `ELU` as the activation because it avoids dying relu problem but also performed well as compared to LeakyRelu\n    atleast in this case. `he_normal` kernel initializer is used as it suits ELU. BatchNormalization is also used for better\n    results.\n    \"\"\"\n    net = Sequential(name='DCNN')\n\n    net.add(\n        Conv2D(\n            filters=64,\n            kernel_size=(3,3),\n            input_shape=input_shape,\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_1'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_1'))\n    net.add(\n        Conv2D(\n            filters=64,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_2'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_2'))\n    \n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))\n    net.add(Dropout(0.45, name='dropout_1'))\n\n    net.add(\n        Conv2D(\n            filters=128,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_3'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_3'))\n    net.add(\n        Conv2D(\n            filters=128,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_4'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_4'))\n    \n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))\n    net.add(Dropout(0.45, name='dropout_2'))\n\n    net.add(\n        Conv2D(\n            filters=256,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_5'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_5'))\n    net.add(\n        Conv2D(\n            filters=256,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_6'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_6'))\n    \n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_3'))\n    net.add(Dropout(0.4, name='dropout_3'))\n\n    net.add(\n        Conv2D(\n            filters=512,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_7'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_7'))\n    net.add(\n        Conv2D(\n            filters=512,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_8'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_8'))\n    \n    net.add(Dropout(0.4, name='dropout_4'))\n    \n    net.add(GlobalMaxPool2D(name=\"globalmax2d\"))\n    \n    if show_arch:\n        net.summary()\n    \n    return net","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Reshape, Lambda\nimport tensorflow.keras.backend as K\n\ndef memory_model(input_shape, num_class, show_arch=True):\n    dcnn = build_dcnn(input_shape, show_arch=False)\n    \n    model = Sequential(name=\"convolutional_Bidrectional_LSTM\")\n\n    model.add(dcnn)\n    model.add(Lambda(lambda x: K.reshape(x, (-1, 1, 512))))\n    \n    model.add(Bidirectional(LSTM(128, return_sequences=True, name=\"bidirect_lstm_1\")))\n    model.add(Dropout(.35, name=\"dropout_1\"))\n    model.add(Bidirectional(LSTM(64, return_sequences=False, name=\"bidirect_lstm_2\")))\n    model.add(Dropout(.45, name=\"dropout_2\"))\n\n    model.add(\n        Dense(\n            128,\n            activation='elu',\n            kernel_initializer='he_normal',\n            name='dense_1'\n        )\n    )\n    model.add(BatchNormalization(name='batchnorm_1'))\n    model.add(Dropout(.7, name=\"dropout_3\"))\n\n    model.add(\n        Dense(\n            num_class,\n            activation='softmax',\n            name='out_layer'\n        )\n    )\n    \n    if show_arch:\n        model.summary()\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.00005,\n    patience=12,\n    verbose=1,\n    restore_best_weights=True,\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_accuracy',\n    factor=0.8,\n    patience=7,\n    min_lr=1e-7,\n    verbose=1,\n)\n\ncallbacks = [\n#     early_stopping,\n    lr_scheduler,\n]\n\nbatch_size = 128\nepochs = 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_SHAPE = (48, 48, 1)\noptim = optimizers.Nadam(0.001)\n\nmodel = memory_model(INPUT_SHAPE, num_class=7)\nmodel.compile(\n        loss='categorical_crossentropy',\n        optimizer=optim,\n        metrics=['accuracy']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import model_from_json\n\nmodel.load_weights(\"/kaggle/input/model-68/model_ck_fer.h5\")\nhistory = model.fit(\n    x=X_train,\n    y=X_test,\n    validation_data = (y_train,y_test),\n    batch_size=batch_size,\n    epochs=1000,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(history.epoch, history.history['accuracy'], label='train')\nsns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\n\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(history.epoch, history.history['loss'], label='train')\nsns.lineplot(history.epoch, history.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history.png')\npyplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`The fluctuations in the epoch metrics is due to the fact that we have very low data for such a complex task.`","metadata":{}},{"cell_type":"code","source":"model.load_weights(\"/kaggle/input/model-68/model_ck_fer.h5\")\nyhat_valid = model.predict_classes(y_train)\nscikitplot.metrics.plot_confusion_matrix(np.argmax(y_test, axis=1), yhat_valid, figsize=(7,7))\npyplot.savefig(\"confusion_matrix.png\")\n\nprint(f'total wrong validation predictions: {np.sum(np.argmax(y_test, axis=1) != yhat_valid)}/{len(yhat_valid)}\\n\\n')\naccuracy = ((np.sum(np.argmax(y_test, axis=1) != yhat_valid))/len(yhat_valid))*100\nprint(f'acurracy: {accuracy}\\n\\n')\nprint(classification_report(np.argmax(y_test, axis=1), yhat_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`If we have more data to train then we will get better and more generalized model.`","metadata":{}},{"cell_type":"code","source":"np.random.seed(0)\nindices = np.random.choice(range(y_train.shape[0]), size=15, replace=False)\n\nfig = pyplot.figure(1, (9,30))\n\ni = 0\nfor idx in indices:\n    true_emotion = FER13_EMOTIONS[np.argmax(y_test[idx])]\n    pred_emotion = FER13_EMOTIONS[model.predict_classes(np.expand_dims(y_train[idx], axis=0))[0]]\n    \n    i += 1\n    ax = pyplot.subplot(15,3,i)\n    sample_img = y_train[idx,0,:,:,0]\n    ax.imshow(sample_img, cmap='gray')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(f\"t:{true_emotion}, p:{pred_emotion}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n  json_file.write(model_json)\nmodel.save_weights(\"model.h5\")\nprint(\"saved model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}